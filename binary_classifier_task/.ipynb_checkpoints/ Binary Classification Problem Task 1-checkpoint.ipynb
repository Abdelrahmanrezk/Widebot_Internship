{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "# Text Pre-Processing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "import re\n",
    "\n",
    "# ML Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Keras \n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Because reading data as one string and some of other issues related to the data in the file, I just read each line of the file to handle the cases and replace like [;,] and for each line retrive the data its contain, then make a new file that contain a cleaned csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def csv_file(file_path):\n",
    "    '''\n",
    "    Argument:\n",
    "        file_path the directory of the file we need to read\n",
    "    return:\n",
    "        list of all rows in the file\n",
    "    '''\n",
    "    df_data = []\n",
    "    with open(file_path, \"r\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader) # escape header\n",
    "        for line in reader:\n",
    "            # before preprocess\n",
    "            line = \",\".join(line) \n",
    "            line = line.replace(';', ' ')\n",
    "            line = line.replace(',', ' ')\n",
    "            line = line.split(' ')\n",
    "            # after preprocess the file\n",
    "            df_data.append(line)\n",
    "            \n",
    "    return df_data\n",
    "df_training_data = csv_file('csv_files/training.csv')\n",
    "df_validation_data = csv_file('csv_files/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After pre-process the lines of the file make a pre-defined cols names\n",
    "cols = []\n",
    "for i in range(len(df_training_data[0])-4):\n",
    "    cols.append(\"variable\" + str(i+1))\n",
    "cols.append('classLabel')\n",
    "cols.append('target1')\n",
    "cols.append('target2')\n",
    "cols.append('target3')\n",
    "df_training_data = pd.DataFrame(df_training_data, columns=cols)\n",
    "df_validation_data = pd.DataFrame(df_validation_data, columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note !\n",
    "\n",
    "- drop the class label because it has no meaning when we train the model\n",
    "- while its have no meaning maybe cause to miss leading our training to the wrong way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3700\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_training_data = df_training_data.drop(['classLabel'], axis=1)\n",
    "df_validation_data = df_validation_data.drop(['classLabel'], axis=1)\n",
    "print(len(df_training_data))\n",
    "print(len(df_validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Class\n",
    "I noticed that the last three columns are the target variables and they like to complete each other so I will follow these steps of pre process the target variables:\n",
    "- use the second one of them as our main target variables\n",
    "- if its empty or Nan value:\n",
    "    - check with the last columns if has the value\n",
    "    - if not I will take the value of first columns\n",
    "    - if still not I will take the prvious value and applied to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\"t\"', '0', '\"no.\"'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_targets = list(df_training_data[['target1', 'target2', 'target3']].values)\n",
    "validation_targets = list(df_validation_data[['target1', 'target2', 'target3']].values)\n",
    "training_targets[0] # each index is three values as 'target1', 'target2', 'target3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def handle_target_class(targets):\n",
    "    '''\n",
    "    Argument:\n",
    "        targets: list of list each of them are 3 values\n",
    "    reuturn:\n",
    "        output_class: One list that contain all of our classes and fill the empty or nan values\n",
    "    '''\n",
    "    output_class = []\n",
    "    for i, target in enumerate(targets):\n",
    "        if target[1] == '\"no.\"' or target[1] == 0 or target[1] == '0':\n",
    "            output_class.append(0)\n",
    "            target[1] = 0\n",
    "        elif target[1] == '\"yes.\"' or target[1] == 1 or target[1] == '1':\n",
    "            output_class.append(1)\n",
    "        elif target[2] == '\"no.\"' or target[2] == 0 or target[2] == '0':\n",
    "                output_class.append(0)\n",
    "        elif target[2] == '\"yes.\"' or target[2] == 1 or target[2] == '1':\n",
    "            output_class.append(1)\n",
    "        elif target[0] == '\"no.\"' or target[0] == 0 or target[0] == '0':\n",
    "                output_class.append(0)\n",
    "        elif target[0] == '\"yes.\"' or target[0] == 1 or target[0] == '1':\n",
    "            output_class.append(1)\n",
    "        else:\n",
    "            output_class.append(output_class[i-1])\n",
    "    return output_class\n",
    "\n",
    "y_train = handle_target_class(training_targets) # The target classes\n",
    "y_test = handle_target_class(validation_targets) # The target classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nan Values\n",
    "\n",
    "Some of our features represent continuous values and beside of that some of these values are **Nan** values so I replace the nan values  by the mean static value or by the most frequent value based on that if the unique values > 100 then replaced with the mean else replace by the most frequent value of the unique values in this feature, then I will apply the features scaling for these features to make the values of our data in some ranges bwtween [0 - 1] or [-1 - 1], which help the model to learn well than of different ranges, and this also avoid the overfitting.\n",
    "\n",
    "Some of other features take the same process but replacd by the most frequent.\n",
    "\n",
    "- Extract continuous features\n",
    "- replace nan values with static mean\n",
    "- features scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note !\n",
    "\n",
    "some of the features have values at same time of some chars so I will replace chars with nan value.\n",
    "\n",
    " then when apply the Imputer these values will change to mean or most frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def handle_char_cell(col):\n",
    "#     '''\n",
    "#     Try to check where its float number if not then give it a nan value\n",
    "#     then using the Imputer_missing functoin will change nan to the mean or most frequent values.\n",
    "#     '''\n",
    "#     for i in range(len(col)):\n",
    "#         try:\n",
    "#             float(col[i])\n",
    "#         except:\n",
    "#             col[i] = np.nan\n",
    "#     return col\n",
    "\n",
    "# def handle_num_cell(col):\n",
    "#     for i in range(len(col)):\n",
    "#         try:\n",
    "#             float(col[i])\n",
    "#             col[i] = np.nan\n",
    "#         except:\n",
    "#             pass\n",
    "#     return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Imputer_missing(features, missing_value, strategy):\n",
    "    '''\n",
    "    Argument:\n",
    "    features: The columns features you will apply the missing values on\n",
    "    missing_value: which is string [nan or string of missing values you need to replace ]\n",
    "    strategy: Replace nan values with mean or with most_frequent values\n",
    "    \n",
    "    Return:\n",
    "    features: The columns with all missing are filled\n",
    "        \n",
    "    '''\n",
    "    imp_mean = SimpleImputer(missing_values=missing_value, strategy=strategy)\n",
    "    for i in range(features.shape[1]):\n",
    "        imp = features[:, i]\n",
    "        imp = np.array(imp).reshape(-1, 1)\n",
    "        imp_mean.fit(imp)\n",
    "        imp = imp_mean.transform(imp)\n",
    "        features[:, i] = imp.reshape(-1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note !\n",
    "\n",
    "After what we have done with continuos values we need to deal with categorial features so we use label encoder from sklearn to encode the different discrete values to some index based on number of classes in the category we are in, and it takes a sequence of index from 0 to number of classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_encoder(categircal_features):\n",
    "    '''\n",
    "    Argument:\n",
    "    categircal_features: the columns that contain categircal features\n",
    "    '''\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for i in range(categircal_features.shape[1]):\n",
    "        categircal_feature = categircal_features[:, i]\n",
    "        le.fit(categircal_feature)\n",
    "        categircal_feature = le.transform(categircal_feature)\n",
    "        categircal_features[:, i] = categircal_feature.reshape(-1)\n",
    "    return categircal_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note !\n",
    "\n",
    "After what we have done our function now its time to apply the process of pre-process using the functions we have implemented and check with training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique values of column variable1 3\n",
      "The number of unique values of column variable2 56\n",
      "The number of unique values of column variable3 24\n",
      "The number of unique values of column variable4 51\n",
      "The number of unique values of column variable5 153\n",
      "The number of unique values of column variable6 17\n",
      "The number of unique values of column variable7 20\n",
      "The number of unique values of column variable8 29\n",
      "The number of unique values of column variable9 30\n",
      "The number of unique values of column variable10 37\n",
      "The number of unique values of column variable11 29\n",
      "The number of unique values of column variable12 10\n",
      "The number of unique values of column variable13 23\n",
      "The number of unique values of column variable14 26\n",
      "The number of unique values of column variable15 37\n",
      "The number of unique values of column variable16 82\n",
      "The number of unique values of column variable17 177\n",
      "The number of unique values of column variable18 186\n",
      "The number of unique values of column target1 8\n",
      "The number of unique values of column target2 5\n",
      "The number of unique values of column target3 3\n"
     ]
    }
   ],
   "source": [
    "for col in df_training_data.columns:\n",
    "    print(\"The number of unique values of column \" + str(col) + \" \" + str(len(pd.unique(df_training_data[col]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tr Pre-process\n",
    "\n",
    "cols1 = ['variable5', 'variable16', 'variable17', 'variable18']\n",
    "\n",
    "cols2 = ['variable1', 'variable2', 'variable3', 'variable4', 'variable6', 'variable7', 'variable8','variable9',\n",
    "       'variable10', 'variable11', 'variable12', 'variable13', 'variable14', 'variable15']\n",
    "\n",
    "# impute the categorical for training data\n",
    "tr_categorical_features = np.array(list(df_training_data[cols2].values)).reshape(-1,14)\n",
    "tr_categorical_features = label_encoder(tr_categorical_features)\n",
    "tr_categorical_features = Imputer_missing(tr_categorical_features, np.nan, \"most_frequent\")\n",
    "\n",
    "# impute the categorical for testing data\n",
    "va_categorical_features = np.array(list(df_validation_data[cols2].values)).reshape(-1,14)\n",
    "va_categorical_features = label_encoder(va_categorical_features)\n",
    "va_categorical_features = Imputer_missing(va_categorical_features, np.nan, \"most_frequent\")\n",
    "\n",
    "# impute the continuous for training data\n",
    "tr_con_features_mean = np.array(list(df_training_data[cols1].values)).reshape(-1,4)\n",
    "tr_con_features_mean = label_encoder(tr_con_features_mean)\n",
    "tr_con_features_mean = Imputer_missing(tr_con_features_mean, np.nan, \"mean\")\n",
    "\n",
    "# impute the continuous for testing data\n",
    "va_con_features_mean = np.array(list(df_validation_data[cols1].values)).reshape(-1,4)\n",
    "va_con_features_mean = label_encoder(va_con_features_mean)\n",
    "va_con_features_mean = Imputer_missing(va_con_features_mean, np.nan, \"mean\")\n",
    "\n",
    "\n",
    "tr_con_features_mean = tr_con_features_mean.astype('float')\n",
    "va_con_features_mean = va_con_features_mean.astype('float')\n",
    "\n",
    "\n",
    "# scaling\n",
    "scale = preprocessing.MinMaxScaler().fit(tr_con_features_mean)\n",
    "tr_con_features_mean = scale.transform(tr_con_features_mean)\n",
    "\n",
    "# scaling\n",
    "scale = preprocessing.MinMaxScaler().fit(va_con_features_mean)\n",
    "va_con_features_mean = scale.transform(va_con_features_mean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# continuous  features\n",
    "tr_con_features_mean = pd.DataFrame(tr_con_features_mean)\n",
    "va_con_features_mean = pd.DataFrame(va_con_features_mean)\n",
    "\n",
    "# categorical features\n",
    "va_categorical_features = pd.DataFrame(va_categorical_features)\n",
    "\n",
    "tr_categorical_features = pd.DataFrame(tr_categorical_features)\n",
    "# Concat continuous and categorical features in one dataframe for training and testing\n",
    "df_training_data = pd.concat([tr_con_features_mean, tr_categorical_features], axis= 1)\n",
    "df_validation_data = pd.concat([va_con_features_mean, va_categorical_features], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.940789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.767568</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>37</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.927632</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>0.323864</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>32</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.046053</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.306818</td>\n",
       "      <td>0.983784</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.072368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.102703</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.392045</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3  0   1   2   3   4   5   6  7  8   \\\n",
       "0  0.940789  0.000000  0.937500  0.767568  1   3  23  37  13   8   3  9  5   \n",
       "1  0.927632  0.024691  0.323864  0.010811  2   2  23  32  15  15  12  9  2   \n",
       "2  0.046053  0.962963  0.306818  0.983784  2  17   6   2  13   8   8  3  2   \n",
       "3  0.072368  0.000000  0.011364  0.102703  1  34   3   2  13   8  10  7  2   \n",
       "4  0.263158  0.000000  0.392045  0.010811  2  18   8   2  13   8  12  9  2   \n",
       "\n",
       "   9  10 11 12 13  \n",
       "0  25  0  2  6  4  \n",
       "1  11  0  0  5  0  \n",
       "2   0  1  4  0  1  \n",
       "3  13  0  0  5  0  \n",
       "4  18  0  0  5  4  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now Display and save the new CSV file after we have done the pre-process Approach\n",
    "df_training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.021053</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.031579</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.463158</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.622449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.568421</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1     2         3  0   1   2   3  4   5  6  7  8   9  10  \\\n",
       "0  0.510204  0.026316  0.67  0.021053  2  17   8   2  6   7  7  0  5  15  1   \n",
       "1  0.163265  0.000000  0.16  0.031579  2   8  12   2  6   7  3  5  2  14  0   \n",
       "2  0.918367  0.000000  0.40  0.463158  2  21   9  28  7  11  5  5  2  15  0   \n",
       "3  0.622449  0.000000  0.08  0.568421  2   3   9   2  7  11  1  5  2   4  1   \n",
       "4  0.755102  0.000000  0.08  0.684211  2   9  11   2  7  11  1  5  2   2  0   \n",
       "\n",
       "  11 12 13  \n",
       "0  0  5  3  \n",
       "1  0  5  3  \n",
       "2  0  5  0  \n",
       "3  0  5  0  \n",
       "4  0  5  3  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3700\n"
     ]
    }
   ],
   "source": [
    "print(len(df_training_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note !\n",
    "\n",
    "Now we have done all of the pre process to our data, next we need to save our new form of the data in csv file along with the Traget labels we have also done above.\n",
    "\n",
    "Beofre of save the new cleaned files I will shuffle the data to be randomies when the model start to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_data['target'] = y_train\n",
    "df_validation_data['target'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fea1', 'fea2', 'fea3', 'fea4', 'fea5', 'fea6', 'fea7', 'fea8', 'fea9', 'fea10', 'fea11', 'fea12', 'fea13', 'fea14', 'fea15', 'fea16', 'fea17', 'fea18', 'target']\n"
     ]
    }
   ],
   "source": [
    "# fea = features\n",
    "cols = []\n",
    "for i in range(len(df_training_data.columns)-1):\n",
    "    cols.append(\"fea\" + str(i+1))\n",
    "cols.append('target')\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle before save as csv\n",
    "df_training_data = df_training_data.sample(frac=1).reset_index(drop=True) # shuffel\n",
    "df_validation_data = df_validation_data.sample(frac=1).reset_index(drop=True) # shuffel\n",
    "\n",
    "#Save as CSV\n",
    "df_training_data.to_csv('csv_files/pre_process_training.csv', header=cols, index=False)\n",
    "df_validation_data.to_csv('csv_files/pre_process_validation.csv', header=cols, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training_data = pd.read_csv('csv_files/pre_process_training.csv')\n",
    "df_validation_data = pd.read_csv('csv_files/pre_process_validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display The Data after The Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fea1</th>\n",
       "      <th>fea2</th>\n",
       "      <th>fea3</th>\n",
       "      <th>fea4</th>\n",
       "      <th>fea5</th>\n",
       "      <th>fea6</th>\n",
       "      <th>fea7</th>\n",
       "      <th>fea8</th>\n",
       "      <th>fea9</th>\n",
       "      <th>fea10</th>\n",
       "      <th>fea11</th>\n",
       "      <th>fea12</th>\n",
       "      <th>fea13</th>\n",
       "      <th>fea14</th>\n",
       "      <th>fea15</th>\n",
       "      <th>fea16</th>\n",
       "      <th>fea17</th>\n",
       "      <th>fea18</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.302632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380682</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.940789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.948864</td>\n",
       "      <td>0.843243</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>37</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.328947</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fea1      fea2      fea3      fea4  fea5  fea6  fea7  fea8  fea9  \\\n",
       "0  0.013158  0.037037  0.011364  1.000000     2    11     3    42     6   \n",
       "1  0.302632  0.000000  0.380682  0.010811     2    34    12     2    13   \n",
       "2  0.940789  0.000000  0.948864  0.843243     2     7    23    37    15   \n",
       "3  0.328947  0.024691  0.011364  0.010811     2    28    22     2    13   \n",
       "4  0.789474  0.037037  0.136364  0.010811     1    14     1     2    15   \n",
       "\n",
       "   fea10  fea11  fea12  fea13  fea14  fea15  fea16  fea17  fea18  target  \n",
       "0      3     17     13      1      1      7      0      1      5       1  \n",
       "1      8     13      9      2      6      1      0      5      4       1  \n",
       "2     15     18      9      2      3      1      2      6      4       1  \n",
       "3      8     15      9     25     20      1      0      5      0       1  \n",
       "4     15      7     10      2      1      0      3      0      1       1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fea1</th>\n",
       "      <th>fea2</th>\n",
       "      <th>fea3</th>\n",
       "      <th>fea4</th>\n",
       "      <th>fea5</th>\n",
       "      <th>fea6</th>\n",
       "      <th>fea7</th>\n",
       "      <th>fea8</th>\n",
       "      <th>fea9</th>\n",
       "      <th>fea10</th>\n",
       "      <th>fea11</th>\n",
       "      <th>fea12</th>\n",
       "      <th>fea13</th>\n",
       "      <th>fea14</th>\n",
       "      <th>fea15</th>\n",
       "      <th>fea16</th>\n",
       "      <th>fea17</th>\n",
       "      <th>fea18</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.632653</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.418367</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.021053</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.234694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.021053</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fea1      fea2  fea3      fea4  fea5  fea6  fea7  fea8  fea9  fea10  \\\n",
       "0  0.040816  0.842105  0.04  0.800000     2    27     1     2     6      7   \n",
       "1  0.632653  0.263158  0.49  0.200000     1     5     9     2     7     11   \n",
       "2  0.102041  0.763158  0.55  0.631579     2    12    16     2     6      7   \n",
       "3  0.418367  0.026316  0.93  0.021053     2    20     3     2     6      7   \n",
       "4  0.234694  0.000000  0.81  0.021053     2    16    16     2     6      7   \n",
       "\n",
       "   fea11  fea12  fea13  fea14  fea15  fea16  fea17  fea18  target  \n",
       "0      0      5     17      1      1     11      4      1       1  \n",
       "1     16      2      2      0      0      2      4      1       0  \n",
       "2      0      5      9      1      1      4      4      1       1  \n",
       "3     16      2     17     18      0      0      5      3       1  \n",
       "4      1      5     20     13      1      0      5      3       0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fea1</th>\n",
       "      <th>fea2</th>\n",
       "      <th>fea3</th>\n",
       "      <th>fea4</th>\n",
       "      <th>fea5</th>\n",
       "      <th>fea6</th>\n",
       "      <th>fea7</th>\n",
       "      <th>fea8</th>\n",
       "      <th>fea9</th>\n",
       "      <th>fea10</th>\n",
       "      <th>fea11</th>\n",
       "      <th>fea12</th>\n",
       "      <th>fea13</th>\n",
       "      <th>fea14</th>\n",
       "      <th>fea15</th>\n",
       "      <th>fea16</th>\n",
       "      <th>fea17</th>\n",
       "      <th>fea18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.302632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380682</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.940789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.948864</td>\n",
       "      <td>0.843243</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>37</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.328947</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fea1      fea2      fea3      fea4  fea5  fea6  fea7  fea8  fea9  \\\n",
       "0  0.013158  0.037037  0.011364  1.000000     2    11     3    42     6   \n",
       "1  0.302632  0.000000  0.380682  0.010811     2    34    12     2    13   \n",
       "2  0.940789  0.000000  0.948864  0.843243     2     7    23    37    15   \n",
       "3  0.328947  0.024691  0.011364  0.010811     2    28    22     2    13   \n",
       "4  0.789474  0.037037  0.136364  0.010811     1    14     1     2    15   \n",
       "\n",
       "   fea10  fea11  fea12  fea13  fea14  fea15  fea16  fea17  fea18  \n",
       "0      3     17     13      1      1      7      0      1      5  \n",
       "1      8     13      9      2      6      1      0      5      4  \n",
       "2     15     18      9      2      3      1      2      6      4  \n",
       "3      8     15      9     25     20      1      0      5      0  \n",
       "4     15      7     10      2      1      0      3      0      1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data\n",
    "training_data = df_training_data.iloc[:, :-1]\n",
    "y_train = df_training_data.iloc[:, -1]\n",
    "\n",
    "# Testing data\n",
    "testing_data = df_validation_data.iloc[:, :-1]\n",
    "y_test = df_validation_data.iloc[:, -1]\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/abdelrahman/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Multinomial nive bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of our training data is:  0.7286486486486486\n",
      "F1 score of our testing data is:  0.47500000000000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "clf_MultinomialNB = MultinomialNB()\n",
    "model = clf_MultinomialNB.fit(training_data, y_train)\n",
    "predict = model.predict(training_data)\n",
    "print(\"F1 score of our training data is: \", f1_score(y_train, predict, average='micro'))\n",
    "predict = model.predict(testing_data)\n",
    "print(\"F1 score of our testing data is: \", f1_score(y_test, predict, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of our training data is:  0.9283783783783783\n",
      "F1 score of our testing data is:  0.47500000000000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/abdelrahman/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_LogisticRegression = LogisticRegression(penalty='l2', solver='sag')\n",
    "logistic_model = clf_LogisticRegression.fit(training_data, y_train)\n",
    "predict = logistic_model.predict(training_data)\n",
    "print(\"F1 score of our training data is: \", f1_score(y_train, predict, average='micro'))\n",
    "predict = model.predict(testing_data)\n",
    "print(\"F1 score of our testing data is: \", f1_score(y_test, predict, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training With Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tensor_model(x_train,y_trains, x_val, y_val, input_shape):\n",
    "  # Sequential model\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(32, activation='relu', input_shape=(input_shape,)))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "                  loss=losses.binary_crossentropy,\n",
    "                  metrics=[metrics.binary_accuracy])\n",
    "    \n",
    "    history = model.fit(x_train,\n",
    "                        y_trains,\n",
    "                        epochs=25,\n",
    "                        batch_size=1024,\n",
    "                        validation_data=(x_val, y_val))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "4/4 [==============================] - 0s 58ms/step - loss: 0.4647 - binary_accuracy: 0.8286 - val_loss: 1.4336 - val_binary_accuracy: 0.5300\n",
      "Epoch 2/25\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.3161 - binary_accuracy: 0.9116 - val_loss: 1.3384 - val_binary_accuracy: 0.5350\n",
      "Epoch 3/25\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.2934 - binary_accuracy: 0.9143 - val_loss: 1.2926 - val_binary_accuracy: 0.5350\n",
      "Epoch 4/25\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.2784 - binary_accuracy: 0.9149 - val_loss: 1.3102 - val_binary_accuracy: 0.5300\n",
      "Epoch 5/25\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.2678 - binary_accuracy: 0.9192 - val_loss: 1.2579 - val_binary_accuracy: 0.5200\n",
      "Epoch 6/25\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.2592 - binary_accuracy: 0.9200 - val_loss: 1.2512 - val_binary_accuracy: 0.5250\n",
      "Epoch 7/25\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.2518 - binary_accuracy: 0.9205 - val_loss: 1.2551 - val_binary_accuracy: 0.5200\n",
      "Epoch 8/25\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.2449 - binary_accuracy: 0.9254 - val_loss: 1.1373 - val_binary_accuracy: 0.5150\n",
      "Epoch 9/25\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.2368 - binary_accuracy: 0.9257 - val_loss: 1.1587 - val_binary_accuracy: 0.5250\n",
      "Epoch 10/25\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2339 - binary_accuracy: 0.9284 - val_loss: 1.1903 - val_binary_accuracy: 0.5150\n",
      "Epoch 11/25\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2247 - binary_accuracy: 0.9276 - val_loss: 1.0970 - val_binary_accuracy: 0.5250\n",
      "Epoch 12/25\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.2274 - binary_accuracy: 0.9281 - val_loss: 1.0539 - val_binary_accuracy: 0.5150\n",
      "Epoch 13/25\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.2188 - binary_accuracy: 0.9278 - val_loss: 1.2598 - val_binary_accuracy: 0.5050\n",
      "Epoch 14/25\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.2154 - binary_accuracy: 0.9295 - val_loss: 1.1255 - val_binary_accuracy: 0.5100\n",
      "Epoch 15/25\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.2095 - binary_accuracy: 0.9303 - val_loss: 1.1223 - val_binary_accuracy: 0.5150\n",
      "Epoch 16/25\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.2050 - binary_accuracy: 0.9300 - val_loss: 0.9609 - val_binary_accuracy: 0.5150\n",
      "Epoch 17/25\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.2126 - binary_accuracy: 0.9295 - val_loss: 1.1210 - val_binary_accuracy: 0.5100\n",
      "Epoch 18/25\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.1985 - binary_accuracy: 0.9311 - val_loss: 1.0948 - val_binary_accuracy: 0.5150\n",
      "Epoch 19/25\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.1971 - binary_accuracy: 0.9324 - val_loss: 1.1294 - val_binary_accuracy: 0.5050\n",
      "Epoch 20/25\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.1957 - binary_accuracy: 0.9327 - val_loss: 1.1253 - val_binary_accuracy: 0.5000\n",
      "Epoch 21/25\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.1941 - binary_accuracy: 0.9316 - val_loss: 1.2467 - val_binary_accuracy: 0.5050\n",
      "Epoch 22/25\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.1892 - binary_accuracy: 0.9335 - val_loss: 1.1197 - val_binary_accuracy: 0.5100\n",
      "Epoch 23/25\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.1860 - binary_accuracy: 0.9346 - val_loss: 1.2003 - val_binary_accuracy: 0.5100\n",
      "Epoch 24/25\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.1957 - binary_accuracy: 0.9332 - val_loss: 1.1728 - val_binary_accuracy: 0.5100\n",
      "Epoch 25/25\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.1837 - binary_accuracy: 0.9335 - val_loss: 1.0521 - val_binary_accuracy: 0.5100\n"
     ]
    }
   ],
   "source": [
    "history = tensor_model(training_data, y_train, testing_data,y_test, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmP8P3Ddsqujfj9EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4686c01860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "\n",
    "\n",
    "acc = history.history['binary_accuracy']\n",
    "val_acc = history.history['val_binary_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKYiISExJQUREYkoKIiISU1IQEZHY/weojDtSECcM+gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4686c01f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # clear figure\n",
    "acc_values = history_dict['binary_accuracy']\n",
    "val_acc_values = history_dict['val_binary_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "As we can see that the model is overfitting on the training data while not doing well on the testing data, and the accuracy is less that 50% which not good and this return to some paramters:\n",
    "\n",
    "- The number of training data is so small\n",
    "- The features of the data has  no meaning to know which feature can affect the training or not\n",
    "- The data contain a lot of empty values\n",
    "- The data contain different values as number and chars and string in some features\n",
    "- The data contain a lot of Nan values\n",
    "\n",
    "\n",
    "**So Some of the ways we can improve our model result can be:**\n",
    "\n",
    "- Increase the size of the data\n",
    "- applying meaning full features as meta-data\n",
    "- Label the data by some expertise\n",
    "- Applying more pre-processing \n",
    "- trying to combine features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
