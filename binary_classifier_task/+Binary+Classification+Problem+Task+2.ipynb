{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "# Text Pre-Processing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "import re\n",
    "\n",
    "# ML Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Keras \n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Because reading data as one string and some of other issues related to the data in the file, I just read each line of the file to handle the cases and replace like [;,] and for each line retrive the data its contain, then make a new file that contain a cleaned csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def csv_file(file_path):\n",
    "    '''\n",
    "    Argument:\n",
    "        file_path the directory of the file we need to read\n",
    "    return:\n",
    "        list of all rows in the file\n",
    "    '''\n",
    "    df_data = []\n",
    "    with open(file_path, \"r\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader) # escape header\n",
    "        for line in reader:\n",
    "            # before preprocess\n",
    "            line = \",\".join(line) \n",
    "            line = line.replace(';', ' ')\n",
    "            line = line.replace(',', ' ')\n",
    "            line = line.split(' ')\n",
    "            # after preprocess the file\n",
    "            df_data.append(line)\n",
    "            \n",
    "    return df_data\n",
    "df_training_data = csv_file('csv_files/training.csv')\n",
    "df_validation_data = csv_file('csv_files/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# After pre-process the lines of the file make a pre-defined cols names\n",
    "cols = []\n",
    "for i in range(len(df_training_data[0])-4):\n",
    "    cols.append(\"variable\" + str(i+1))\n",
    "cols.append('classLabel')\n",
    "cols.append('target1')\n",
    "cols.append('target2')\n",
    "cols.append('target3')\n",
    "df_training_data = pd.DataFrame(df_training_data, columns=cols)\n",
    "df_validation_data = pd.DataFrame(df_validation_data, columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note !\n",
    "\n",
    "- drop the class label because it has no meaning when we train the model\n",
    "- while its have no meaning maybe cause to miss leading our training to the wrong way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3700\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_training_data = df_training_data.drop(['classLabel'], axis=1)\n",
    "df_validation_data = df_validation_data.drop(['classLabel'], axis=1)\n",
    "print(len(df_training_data))\n",
    "print(len(df_validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Class\n",
    "I noticed that the last three columns are the target variables and they like to complete each other so I will follow these steps of pre process the target variables:\n",
    "- use the second one of them as our main target variables\n",
    "- if its empty or Nan value:\n",
    "    - check with the last columns if has the value\n",
    "    - if not I will take the value of first columns\n",
    "    - if still not I will take the prvious value and applied to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\"t\"', '0', '\"no.\"'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_targets = list(df_training_data[['target1', 'target2', 'target3']].values)\n",
    "validation_targets = list(df_validation_data[['target1', 'target2', 'target3']].values)\n",
    "training_targets[0] # each index is three values as 'target1', 'target2', 'target3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def handle_target_class(targets):\n",
    "    '''\n",
    "    Argument:\n",
    "        targets: list of list each of them are 3 values\n",
    "    reuturn:\n",
    "        output_class: One list that contain all of our classes and fill the empty or nan values\n",
    "    '''\n",
    "    output_class = []\n",
    "    for i, target in enumerate(targets):\n",
    "        if target[1] == '\"no.\"' or target[1] == 0 or target[1] == '0':\n",
    "            output_class.append(0)\n",
    "            target[1] = 0\n",
    "        elif target[1] == '\"yes.\"' or target[1] == 1 or target[1] == '1':\n",
    "            output_class.append(1)\n",
    "        elif target[2] == '\"no.\"' or target[2] == 0 or target[2] == '0':\n",
    "                output_class.append(0)\n",
    "        elif target[2] == '\"yes.\"' or target[2] == 1 or target[2] == '1':\n",
    "            output_class.append(1)\n",
    "        elif target[0] == '\"no.\"' or target[0] == 0 or target[0] == '0':\n",
    "                output_class.append(0)\n",
    "        elif target[0] == '\"yes.\"' or target[0] == 1 or target[0] == '1':\n",
    "            output_class.append(1)\n",
    "        else:\n",
    "            output_class.append(output_class[i-1])\n",
    "    return output_class\n",
    "\n",
    "y_train = handle_target_class(training_targets) # The target classes\n",
    "y_test = handle_target_class(validation_targets) # The target classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nan Values\n",
    "\n",
    "Some of our features represent continuous values and beside of that some of these values are **Nan** values so I replace the nan values  by the mean static value or by the most frequent value based on that if the unique values > 100 then replaced with the mean else replace by the most frequent value of the unique values in this feature, then I will apply the features scaling for these features to make the values of our data in some ranges bwtween [0 - 1] or [-1 - 1], which help the model to learn well than of different ranges, and this also avoid the overfitting.\n",
    "\n",
    "Some of other features take the same process but replacd by the most frequent.\n",
    "\n",
    "- Extract continuous features\n",
    "- replace nan values with static mean\n",
    "- features scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note !\n",
    "\n",
    "some of the features have values at same time of some chars so I will replace chars with nan value.\n",
    "\n",
    " then when apply the Imputer these values will change to mean or most frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def handle_char_cell(col):\n",
    "#     '''\n",
    "#     Try to check where its float number if not then give it a nan value\n",
    "#     then using the Imputer_missing functoin will change nan to the mean or most frequent values.\n",
    "#     '''\n",
    "#     for i in range(len(col)):\n",
    "#         try:\n",
    "#             float(col[i])\n",
    "#         except:\n",
    "#             col[i] = np.nan\n",
    "#     return col\n",
    "\n",
    "# def handle_num_cell(col):\n",
    "#     for i in range(len(col)):\n",
    "#         try:\n",
    "#             float(col[i])\n",
    "#             col[i] = np.nan\n",
    "#         except:\n",
    "#             pass\n",
    "#     return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Imputer_missing(features, missing_value, strategy):\n",
    "    '''\n",
    "    Argument:\n",
    "    features: The columns features you will apply the missing values on\n",
    "    missing_value: which is string [nan or string of missing values you need to replace ]\n",
    "    strategy: Replace nan values with mean or with most_frequent values\n",
    "    \n",
    "    Return:\n",
    "    features: The columns with all missing are filled\n",
    "        \n",
    "    '''\n",
    "    imp_mean = SimpleImputer(missing_values=missing_value, strategy=strategy)\n",
    "    for i in range(features.shape[1]):\n",
    "        imp = features[:, i]\n",
    "        imp = np.array(imp).reshape(-1, 1)\n",
    "        imp_mean.fit(imp)\n",
    "        imp = imp_mean.transform(imp)\n",
    "        features[:, i] = imp.reshape(-1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note !\n",
    "\n",
    "After what we have done with continuos values we need to deal with categorial features so we use label encoder from sklearn to encode the different discrete values to some index based on number of classes in the category we are in, and it takes a sequence of index from 0 to number of classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_encoder(categircal_features):\n",
    "    '''\n",
    "    Argument:\n",
    "    categircal_features: the columns that contain categircal features\n",
    "    '''\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for i in range(categircal_features.shape[1]):\n",
    "        categircal_feature = categircal_features[:, i]\n",
    "        le.fit(categircal_feature)\n",
    "        categircal_feature = le.transform(categircal_feature)\n",
    "        categircal_features[:, i] = categircal_feature.reshape(-1)\n",
    "    return categircal_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note !\n",
    "\n",
    "After what we have done our function now its time to apply the process of pre-process using the functions we have implemented and check with training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique values of column variable1 3\n",
      "The number of unique values of column variable2 56\n",
      "The number of unique values of column variable3 24\n",
      "The number of unique values of column variable4 51\n",
      "The number of unique values of column variable5 153\n",
      "The number of unique values of column variable6 17\n",
      "The number of unique values of column variable7 20\n",
      "The number of unique values of column variable8 29\n",
      "The number of unique values of column variable9 30\n",
      "The number of unique values of column variable10 37\n",
      "The number of unique values of column variable11 29\n",
      "The number of unique values of column variable12 10\n",
      "The number of unique values of column variable13 23\n",
      "The number of unique values of column variable14 26\n",
      "The number of unique values of column variable15 37\n",
      "The number of unique values of column variable16 82\n",
      "The number of unique values of column variable17 177\n",
      "The number of unique values of column variable18 186\n",
      "The number of unique values of column target1 8\n",
      "The number of unique values of column target2 5\n",
      "The number of unique values of column target3 3\n"
     ]
    }
   ],
   "source": [
    "for col in df_training_data.columns:\n",
    "    print(\"The number of unique values of column \" + str(col) + \" \" + str(len(pd.unique(df_training_data[col]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tr Pre-process\n",
    "\n",
    "cols1 = ['variable5', 'variable16', 'variable17', 'variable18']\n",
    "\n",
    "cols2 = ['variable1', 'variable2', 'variable3', 'variable4', 'variable6', 'variable7', 'variable8','variable9',\n",
    "       'variable10', 'variable11', 'variable12', 'variable13', 'variable14', 'variable15']\n",
    "\n",
    "# impute the categorical for training data\n",
    "tr_categorical_features = np.array(list(df_training_data[cols2].values)).reshape(-1,14)\n",
    "tr_categorical_features = label_encoder(tr_categorical_features)\n",
    "tr_categorical_features = Imputer_missing(tr_categorical_features, np.nan, \"most_frequent\")\n",
    "\n",
    "# impute the categorical for testing data\n",
    "va_categorical_features = np.array(list(df_validation_data[cols2].values)).reshape(-1,14)\n",
    "va_categorical_features = label_encoder(va_categorical_features)\n",
    "va_categorical_features = Imputer_missing(va_categorical_features, np.nan, \"most_frequent\")\n",
    "\n",
    "# impute the continuous for training data\n",
    "tr_con_features_mean = np.array(list(df_training_data[cols1].values)).reshape(-1,4)\n",
    "tr_con_features_mean = label_encoder(tr_con_features_mean)\n",
    "tr_con_features_mean = Imputer_missing(tr_con_features_mean, np.nan, \"mean\")\n",
    "\n",
    "# impute the continuous for testing data\n",
    "va_con_features_mean = np.array(list(df_validation_data[cols1].values)).reshape(-1,4)\n",
    "va_con_features_mean = label_encoder(va_con_features_mean)\n",
    "va_con_features_mean = Imputer_missing(va_con_features_mean, np.nan, \"mean\")\n",
    "\n",
    "\n",
    "tr_con_features_mean = tr_con_features_mean.astype('float')\n",
    "va_con_features_mean = va_con_features_mean.astype('float')\n",
    "\n",
    "\n",
    "# scaling\n",
    "scale = preprocessing.MinMaxScaler().fit(tr_con_features_mean)\n",
    "tr_con_features_mean = scale.transform(tr_con_features_mean)\n",
    "\n",
    "# scaling\n",
    "scale = preprocessing.MinMaxScaler().fit(va_con_features_mean)\n",
    "va_con_features_mean = scale.transform(va_con_features_mean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# continuous  features\n",
    "tr_con_features_mean = pd.DataFrame(tr_con_features_mean)\n",
    "va_con_features_mean = pd.DataFrame(va_con_features_mean)\n",
    "\n",
    "# categorical features\n",
    "va_categorical_features = pd.DataFrame(va_categorical_features)\n",
    "\n",
    "tr_categorical_features = pd.DataFrame(tr_categorical_features)\n",
    "# Concat continuous and categorical features in one dataframe for training and testing\n",
    "df_training_data = pd.concat([tr_con_features_mean, tr_categorical_features], axis= 1)\n",
    "df_validation_data = pd.concat([va_con_features_mean, va_categorical_features], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.940789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.767568</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>37</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.927632</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>0.323864</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>32</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.046053</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.306818</td>\n",
       "      <td>0.983784</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.072368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.102703</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.392045</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3  0   1   2   3   4   5   6  7  8   \\\n",
       "0  0.940789  0.000000  0.937500  0.767568  1   3  23  37  13   8   3  9  5   \n",
       "1  0.927632  0.024691  0.323864  0.010811  2   2  23  32  15  15  12  9  2   \n",
       "2  0.046053  0.962963  0.306818  0.983784  2  17   6   2  13   8   8  3  2   \n",
       "3  0.072368  0.000000  0.011364  0.102703  1  34   3   2  13   8  10  7  2   \n",
       "4  0.263158  0.000000  0.392045  0.010811  2  18   8   2  13   8  12  9  2   \n",
       "\n",
       "   9  10 11 12 13  \n",
       "0  25  0  2  6  4  \n",
       "1  11  0  0  5  0  \n",
       "2   0  1  4  0  1  \n",
       "3  13  0  0  5  0  \n",
       "4  18  0  0  5  4  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now Display and save the new CSV file after we have done the pre-process Approach\n",
    "df_training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.021053</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.031579</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.463158</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.622449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.568421</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1     2         3  0   1   2   3  4   5  6  7  8   9  10  \\\n",
       "0  0.510204  0.026316  0.67  0.021053  2  17   8   2  6   7  7  0  5  15  1   \n",
       "1  0.163265  0.000000  0.16  0.031579  2   8  12   2  6   7  3  5  2  14  0   \n",
       "2  0.918367  0.000000  0.40  0.463158  2  21   9  28  7  11  5  5  2  15  0   \n",
       "3  0.622449  0.000000  0.08  0.568421  2   3   9   2  7  11  1  5  2   4  1   \n",
       "4  0.755102  0.000000  0.08  0.684211  2   9  11   2  7  11  1  5  2   2  0   \n",
       "\n",
       "  11 12 13  \n",
       "0  0  5  3  \n",
       "1  0  5  3  \n",
       "2  0  5  0  \n",
       "3  0  5  0  \n",
       "4  0  5  3  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3700\n"
     ]
    }
   ],
   "source": [
    "print(len(df_training_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note !\n",
    "\n",
    "Now we have done all of the pre process to our data, next we need to save our new form of the data in csv file along with the Traget labels we have also done above.\n",
    "\n",
    "Beofre of save the new cleaned files I will shuffle the data to be randomies when the model start to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training_data['target'] = y_train\n",
    "df_validation_data['target'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fea1', 'fea2', 'fea3', 'fea4', 'fea5', 'fea6', 'fea7', 'fea8', 'fea9', 'fea10', 'fea11', 'fea12', 'fea13', 'fea14', 'fea15', 'fea16', 'fea17', 'fea18', 'target']\n"
     ]
    }
   ],
   "source": [
    "# fea = features\n",
    "cols = []\n",
    "for i in range(len(df_training_data.columns)-1):\n",
    "    cols.append(\"fea\" + str(i+1))\n",
    "cols.append('target')\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle before save as csv\n",
    "df_training_data = df_training_data.sample(frac=1).reset_index(drop=True) # shuffel\n",
    "df_validation_data = df_validation_data.sample(frac=1).reset_index(drop=True) # shuffel\n",
    "\n",
    "#Save as CSV\n",
    "df_training_data.to_csv('csv_files/pre_process_training.csv', header=cols, index=False)\n",
    "df_validation_data.to_csv('csv_files/pre_process_validation.csv', header=cols, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_training_data = pd.read_csv('csv_files/pre_process_training.csv')\n",
    "df_validation_data = pd.read_csv('csv_files/pre_process_validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display The Data after The Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fea1</th>\n",
       "      <th>fea2</th>\n",
       "      <th>fea3</th>\n",
       "      <th>fea4</th>\n",
       "      <th>fea5</th>\n",
       "      <th>fea6</th>\n",
       "      <th>fea7</th>\n",
       "      <th>fea8</th>\n",
       "      <th>fea9</th>\n",
       "      <th>fea10</th>\n",
       "      <th>fea11</th>\n",
       "      <th>fea12</th>\n",
       "      <th>fea13</th>\n",
       "      <th>fea14</th>\n",
       "      <th>fea15</th>\n",
       "      <th>fea16</th>\n",
       "      <th>fea17</th>\n",
       "      <th>fea18</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.345679</td>\n",
       "      <td>0.960227</td>\n",
       "      <td>0.005405</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.605405</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.822368</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.486842</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.440789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fea1      fea2      fea3      fea4  fea5  fea6  fea7  fea8  fea9  \\\n",
       "0  0.013158  0.345679  0.960227  0.005405     2    31     0    18     6   \n",
       "1  0.078947  0.000000  0.011364  0.605405     1    16    10     2    13   \n",
       "2  0.822368  0.037037  0.011364  0.010811     2    53    10     2    13   \n",
       "3  0.486842  0.000000  0.011364  0.918919     2    34     3     2    13   \n",
       "4  0.440789  0.000000  0.011364  0.010811     1    30     3     2    13   \n",
       "\n",
       "   fea10  fea11  fea12  fea13  fea14  fea15  fea16  fea17  fea18  target  \n",
       "0      4      9     17      1      1      3      2      1     34       1  \n",
       "1      8      0      4      2      3      0      2     16      0       0  \n",
       "2      8      8      3      2      1      0      3      4      1       1  \n",
       "3      8      0      4     12     18      1      2     10      0       1  \n",
       "4      8     15      9     31     14      1      2     16      4       1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fea1</th>\n",
       "      <th>fea2</th>\n",
       "      <th>fea3</th>\n",
       "      <th>fea4</th>\n",
       "      <th>fea5</th>\n",
       "      <th>fea6</th>\n",
       "      <th>fea7</th>\n",
       "      <th>fea8</th>\n",
       "      <th>fea9</th>\n",
       "      <th>fea10</th>\n",
       "      <th>fea11</th>\n",
       "      <th>fea12</th>\n",
       "      <th>fea13</th>\n",
       "      <th>fea14</th>\n",
       "      <th>fea15</th>\n",
       "      <th>fea16</th>\n",
       "      <th>fea17</th>\n",
       "      <th>fea18</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.255102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.778947</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.021053</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.021053</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.410526</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.189474</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fea1      fea2  fea3      fea4  fea5  fea6  fea7  fea8  fea9  fea10  \\\n",
       "0  0.255102  0.000000  0.88  0.778947     2    38    11     2     6      7   \n",
       "1  0.734694  0.000000  0.94  0.021053     2    12     9     2     6      7   \n",
       "2  0.020408  0.052632  0.45  0.021053     1    26     8    14     3      8   \n",
       "3  0.469388  0.000000  0.00  0.410526     2    32     1     2     6      7   \n",
       "4  0.795918  0.236842  0.13  0.189474     2    34    16     2     6      7   \n",
       "\n",
       "   fea11  fea12  fea13  fea14  fea15  fea16  fea17  fea18  target  \n",
       "0      0      5      5     18      1      1     14      3       0  \n",
       "1      1      0      2      7      0      0      5      3       1  \n",
       "2      2     11     11      1      0      2      4      1       1  \n",
       "3     12      5      2      2      0      0      5      0       1  \n",
       "4      7      6      2      1      1      8      0      1       0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fea1</th>\n",
       "      <th>fea2</th>\n",
       "      <th>fea3</th>\n",
       "      <th>fea4</th>\n",
       "      <th>fea5</th>\n",
       "      <th>fea6</th>\n",
       "      <th>fea7</th>\n",
       "      <th>fea8</th>\n",
       "      <th>fea9</th>\n",
       "      <th>fea10</th>\n",
       "      <th>fea11</th>\n",
       "      <th>fea12</th>\n",
       "      <th>fea13</th>\n",
       "      <th>fea14</th>\n",
       "      <th>fea15</th>\n",
       "      <th>fea16</th>\n",
       "      <th>fea17</th>\n",
       "      <th>fea18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.345679</td>\n",
       "      <td>0.960227</td>\n",
       "      <td>0.005405</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.605405</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.822368</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.486842</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.440789</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fea1      fea2      fea3      fea4  fea5  fea6  fea7  fea8  fea9  \\\n",
       "0  0.013158  0.345679  0.960227  0.005405     2    31     0    18     6   \n",
       "1  0.078947  0.000000  0.011364  0.605405     1    16    10     2    13   \n",
       "2  0.822368  0.037037  0.011364  0.010811     2    53    10     2    13   \n",
       "3  0.486842  0.000000  0.011364  0.918919     2    34     3     2    13   \n",
       "4  0.440789  0.000000  0.011364  0.010811     1    30     3     2    13   \n",
       "\n",
       "   fea10  fea11  fea12  fea13  fea14  fea15  fea16  fea17  fea18  \n",
       "0      4      9     17      1      1      3      2      1     34  \n",
       "1      8      0      4      2      3      0      2     16      0  \n",
       "2      8      8      3      2      1      0      3      4      1  \n",
       "3      8      0      4     12     18      1      2     10      0  \n",
       "4      8     15      9     31     14      1      2     16      4  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data\n",
    "training_data = df_training_data.iloc[:, :-1]\n",
    "y_train = df_training_data.iloc[:, -1]\n",
    "\n",
    "# Testing data\n",
    "testing_data = df_validation_data.iloc[:, :-1]\n",
    "y_test = df_validation_data.iloc[:, -1]\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/abdelrahman/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Multinomial nive bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of our training data is:  0.7286486486486486\n",
      "F1 score of our testing data is:  0.47500000000000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "clf_MultinomialNB = MultinomialNB()\n",
    "model = clf_MultinomialNB.fit(training_data, y_train)\n",
    "predict = model.predict(training_data)\n",
    "print(\"F1 score of our training data is: \", f1_score(y_train, predict, average='micro'))\n",
    "predict = model.predict(testing_data)\n",
    "print(\"F1 score of our testing data is: \", f1_score(y_test, predict, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of our training data is:  0.9283783783783783\n",
      "F1 score of our testing data is:  0.47500000000000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/abdelrahman/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_LogisticRegression = LogisticRegression(penalty='l2', solver='sag')\n",
    "logistic_model = clf_LogisticRegression.fit(training_data, y_train)\n",
    "predict = logistic_model.predict(training_data)\n",
    "print(\"F1 score of our training data is: \", f1_score(y_train, predict, average='micro'))\n",
    "predict = model.predict(testing_data)\n",
    "print(\"F1 score of our testing data is: \", f1_score(y_test, predict, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training With Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tensor_model(x_train,y_trains, x_val, y_val, input_shape):\n",
    "  # Sequential model\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(32, activation='relu', input_shape=(input_shape,)))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "                  loss=losses.binary_crossentropy,\n",
    "                  metrics=[metrics.binary_accuracy])\n",
    "    \n",
    "    history = model.fit(x_train,\n",
    "                        y_trains,\n",
    "                        epochs=25,\n",
    "                        batch_size=1024,\n",
    "                        validation_data=(x_val, y_val))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.4233 - binary_accuracy: 0.9254 - val_loss: 1.5112 - val_binary_accuracy: 0.5200\n",
      "Epoch 2/25\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.3128 - binary_accuracy: 0.9224 - val_loss: 1.3314 - val_binary_accuracy: 0.5200\n",
      "Epoch 3/25\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.2814 - binary_accuracy: 0.9259 - val_loss: 1.2056 - val_binary_accuracy: 0.5150\n",
      "Epoch 4/25\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2688 - binary_accuracy: 0.9259 - val_loss: 1.2716 - val_binary_accuracy: 0.5100\n",
      "Epoch 5/25\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2540 - binary_accuracy: 0.9257 - val_loss: 1.1905 - val_binary_accuracy: 0.5150\n",
      "Epoch 6/25\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.2462 - binary_accuracy: 0.9259 - val_loss: 1.2636 - val_binary_accuracy: 0.5100\n",
      "Epoch 7/25\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2405 - binary_accuracy: 0.9257 - val_loss: 1.1882 - val_binary_accuracy: 0.5100\n",
      "Epoch 8/25\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.2347 - binary_accuracy: 0.9251 - val_loss: 1.2136 - val_binary_accuracy: 0.5100\n",
      "Epoch 9/25\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.2344 - binary_accuracy: 0.9254 - val_loss: 1.1659 - val_binary_accuracy: 0.5100\n",
      "Epoch 10/25\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.2264 - binary_accuracy: 0.9251 - val_loss: 1.0780 - val_binary_accuracy: 0.5100\n",
      "Epoch 11/25\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2213 - binary_accuracy: 0.9259 - val_loss: 1.0988 - val_binary_accuracy: 0.5100\n",
      "Epoch 12/25\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.2201 - binary_accuracy: 0.9257 - val_loss: 1.0507 - val_binary_accuracy: 0.5100\n",
      "Epoch 13/25\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2155 - binary_accuracy: 0.9257 - val_loss: 1.0337 - val_binary_accuracy: 0.5100\n",
      "Epoch 14/25\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.2223 - binary_accuracy: 0.921 - 0s 16ms/step - loss: 0.2177 - binary_accuracy: 0.9235 - val_loss: 1.0560 - val_binary_accuracy: 0.5100\n",
      "Epoch 15/25\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2106 - binary_accuracy: 0.9249 - val_loss: 1.1364 - val_binary_accuracy: 0.5100\n",
      "Epoch 16/25\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.2094 - binary_accuracy: 0.9262 - val_loss: 1.0907 - val_binary_accuracy: 0.5100\n",
      "Epoch 17/25\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2092 - binary_accuracy: 0.9254 - val_loss: 0.9928 - val_binary_accuracy: 0.5150\n",
      "Epoch 18/25\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2050 - binary_accuracy: 0.9257 - val_loss: 1.0222 - val_binary_accuracy: 0.5000\n",
      "Epoch 19/25\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2015 - binary_accuracy: 0.9257 - val_loss: 1.0748 - val_binary_accuracy: 0.5000\n",
      "Epoch 20/25\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.1983 - binary_accuracy: 0.9268 - val_loss: 1.1232 - val_binary_accuracy: 0.5050\n",
      "Epoch 21/25\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2044 - binary_accuracy: 0.9276 - val_loss: 1.1178 - val_binary_accuracy: 0.5000\n",
      "Epoch 22/25\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.1961 - binary_accuracy: 0.9270 - val_loss: 1.0738 - val_binary_accuracy: 0.5050\n",
      "Epoch 23/25\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.1939 - binary_accuracy: 0.9278 - val_loss: 1.0605 - val_binary_accuracy: 0.5000\n",
      "Epoch 24/25\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.1939 - binary_accuracy: 0.9292 - val_loss: 1.1826 - val_binary_accuracy: 0.5000\n",
      "Epoch 25/25\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.1936 - binary_accuracy: 0.9278 - val_loss: 1.0767 - val_binary_accuracy: 0.5050\n"
     ]
    }
   ],
   "source": [
    "history = tensor_model(training_data, y_train, testing_data,y_test, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\np1OjqI8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd08d72da58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "\n",
    "\n",
    "acc = history.history['binary_accuracy']\n",
    "val_acc = history.history['val_binary_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYGZmGScFMzPL/B/wfNPmSxz+LAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd08df7c780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # clear figure\n",
    "acc_values = history_dict['binary_accuracy']\n",
    "val_acc_values = history_dict['val_binary_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "As we can see that the model is overfitting on the training data while not doing well on the testing data, and the accuracy is less that 50% which not good and this return to some paramters:\n",
    "\n",
    "- The number of training data is so small\n",
    "- The features of the data has  no meaning to know which feature can affect the training or not\n",
    "- The data contain a lot of empty values\n",
    "- The data contain different values as number and chars and string in some features\n",
    "- The data contain a lot of Nan values\n",
    "\n",
    "\n",
    "**So Some of the ways we can improve our model result can be:**\n",
    "\n",
    "- Increase the size of the data\n",
    "- applying meaning full features as meta-data\n",
    "- Label the data by some expertise\n",
    "- Applying more pre-processing \n",
    "- trying to combine features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
